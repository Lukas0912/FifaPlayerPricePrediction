{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIFA PLAYER PRICE PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors :\n",
    "\n",
    "- Daniel FU : fu.2121690@studenti.uniroma1.it / Matricola : 2121690\n",
    "- Marvin BERGER : berger.2117502@studenti.uniroma1.it / Matricola : 2117502\n",
    "- Lukas WEIGMANN : weigmann.2117702@studenti.uniroma1.it / Matricola : 2117702\n",
    "- Srinjan GHOSH : ghosh.2053796@studenti.uniroma1.it / Matricola : 2053796\n",
    "- Agnese LORELLI : lorelli.1966415@studenti.uniroma1.it / Matricola : 1966415"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final project of the course **\"Foundations of Data Science\"**. Our goal is to predict the market value of a player in the game FIFA. We chose this topic because of our interest in soccer and the fact that the data for the FIFA games is very closely related to market data from reality. This way we can develop an approach for predicting players' market values while still having a lot of other data on top for experimenting. We found a Dataset on **Kaggle** : ​​​[FIFA](https://www.kaggle.com/datasets/stefanoleone992/fifa-23-complete-player-dataset?select=male_players.csv). This dataset contains players' characteristics from FIFA 15 to FIFA 23, it has 110 features and over 1 million instances.\n",
    "We want to optimize several models on the prediction of the player price, compare the models considering the resulting Root Mean Squared Error and R^2 values and be able to make conclusion about the importance of the features for such a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team assignments\n",
    "\n",
    "We decided to split the workload :\n",
    "\n",
    "- Daniel FU : XGBoost + Feature Importance\n",
    "- Marvin BERGER : Feature Engineering + Linear Regression\n",
    "- Lukas WEIGMANN : Feature Engineering + Linear Regression + Plot\n",
    "- Srinjan GHOSH : Feature Engineering + Linear and Polynomial Regression\n",
    "- Agnese LORELLI : Decision Tree + Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './male_players.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m DATASET_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./male_players.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m dataset_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dataset_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './male_players.csv'"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"./male_players.csv\"\n",
    "dataset_df = pd.read_csv(DATASET_PATH)\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the columns we have to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to deal with **Feature Engineering** because the dataset has too many features, we need to reduce it:\n",
    "Taking the mean of the related attributes in order to form a summary of each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['attacking_mean'] = np.mean(dataset_df[['attacking_crossing',\n",
    "                                       'attacking_finishing',\n",
    "                                       'attacking_heading_accuracy',\n",
    "                                       'attacking_short_passing',\n",
    "                                       'attacking_volleys']], axis=1)\n",
    "\n",
    "dataset_df['skill_mean'] = np.mean(dataset_df[['skill_dribbling',\n",
    "                                   'skill_curve',\n",
    "                                   'skill_fk_accuracy',\n",
    "                                   'skill_long_passing',\n",
    "                                   'skill_ball_control']], axis=1)\n",
    "\n",
    "dataset_df['movement_mean'] = np.mean(dataset_df[['movement_acceleration',\n",
    "                                      'movement_sprint_speed',\n",
    "                                      'movement_agility',\n",
    "                                      'movement_reactions',\n",
    "                                      'movement_balance']], axis=1)\n",
    "\n",
    "dataset_df['power_mean'] = np.mean(dataset_df[['power_shot_power',\n",
    "                                   'power_jumping',\n",
    "                                   'power_stamina',\n",
    "                                   'power_strength',\n",
    "                                   'power_long_shots']], axis=1)\n",
    "\n",
    "dataset_df['mentality_mean'] = np.mean(dataset_df[['mentality_aggression',\n",
    "                                       'mentality_interceptions',\n",
    "                                       'mentality_positioning',\n",
    "                                       'mentality_vision',\n",
    "                                       'mentality_penalties',\n",
    "                                       'mentality_composure']], axis=1)\n",
    "\n",
    "dataset_df['defending_mean'] = np.mean(dataset_df[['defending_marking_awareness',\n",
    "                                       'defending_standing_tackle',\n",
    "                                       'defending_sliding_tackle']], axis=1)\n",
    "\n",
    "dataset_df['goalkeeping_mean'] = np.mean(dataset_df[['goalkeeping_diving',\n",
    "                                         'goalkeeping_handling',\n",
    "                                         'goalkeeping_kicking',\n",
    "                                         'goalkeeping_positioning',\n",
    "                                         'goalkeeping_reflexes',\n",
    "                                         'goalkeeping_speed']], axis=1)\n",
    "\n",
    "columns_to_remove = ['attacking_crossing', 'attacking_finishing', 'attacking_heading_accuracy',\n",
    "                      'attacking_short_passing', 'attacking_volleys', 'skill_dribbling', 'skill_curve',\n",
    "                      'skill_fk_accuracy', 'skill_long_passing', 'skill_ball_control',\n",
    "                      'movement_acceleration', 'movement_sprint_speed', 'movement_agility',\n",
    "                      'movement_reactions', 'movement_balance', 'power_shot_power', 'power_jumping',\n",
    "                      'power_stamina', 'power_strength', 'power_long_shots', 'mentality_aggression',\n",
    "                      'mentality_interceptions', 'mentality_positioning', 'mentality_vision',\n",
    "                      'mentality_penalties', 'mentality_composure', 'defending_marking_awareness',\n",
    "                      'defending_standing_tackle', 'defending_sliding_tackle', 'goalkeeping_diving',\n",
    "                      'goalkeeping_handling', 'goalkeeping_kicking', 'goalkeeping_positioning',\n",
    "                      'goalkeeping_reflexes', 'goalkeeping_speed']\n",
    "\n",
    "# Remove the original columns\n",
    "dataset_df.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "# Display the modified DataFrame\n",
    "dataset_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use label encoding to encode categorical variables which might decide the market value of a player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['preferred_foot'] = LabelEncoder().fit_transform(dataset_df['preferred_foot'])\n",
    "dataset_df['work_rate'] = LabelEncoder().fit_transform(dataset_df['work_rate'])\n",
    "dataset_df['body_type'] = LabelEncoder().fit_transform(dataset_df['body_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the values of the changed attrbiutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[['preferred_foot', 'work_rate', 'body_type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize how the attributes might be correlated with our dependent variable **value_eur**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_column = 'value_eur'\n",
    "\n",
    "correlations= dataset_df.corrwith(dataset_df[selected_column])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(x=correlations.index, y=correlations.values, palette='viridis')\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title(f'Correlation of Column {selected_column} with Other Columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some features disappeared, it has removed features whose values are String (e.g. player_url, player_face_url ...). We have now **39 features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the uncessary columns above a particular threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keepOnlyDataOverAThreshold(data, selected_column, threshold):\n",
    "    correlations = dataset_df.corrwith(dataset_df[selected_column])\n",
    "    columns_to_keep = correlations[correlations.abs()>threshold].index.to_list()\n",
    "    columns_to_delete = list(set(dataset_df.columns.to_list()) - set(columns_to_keep))\n",
    "    return data.drop(columns=columns_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = keepOnlyDataOverAThreshold(dataset_df, 'value_eur', 0.1)\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove all the rows which have N/A values --> **250k instances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop this feature because it has too much correlation with the target (almost 1) and thus we want to avoid a scenario where we predict a player price basically already using the market value. It has been a debate inside the team if we should keep it or not, but thanks to our results, we know that it is better to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset_df.drop(columns=\"release_clause_eur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "columns = dataset_df.columns.to_list()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "dataset_df[columns] = scaler.fit_transform(dataset_df[columns])\n",
    "\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Various Models\n",
    "\n",
    "We decided to use 4 approaches for our task.\n",
    "First we wanted to use regression. We expect the regression to maybe not perform that well since we have a lot of features that are not really correlated with our label. However we have a lot of features and training data.\n",
    "Furthermore we decided to use regression trees. In the first variant we only want to train one regression tree for prediction and in the second variant a Random Forest of regression trees. Especially the random forest we expect to perform well.\n",
    "Lastly we use a gradient boosting model. Again we expect it to perform very well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's perform the train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset_df[\"value_eur\"]\n",
    "X = dataset_df.drop(\"value_eur\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training data shape: {X_train}\")\n",
    "print(f\"Test data shape: {X_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearReg(X_train, X_test, y_train, y_test):\n",
    "  model = LinearRegression()\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_test)\n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  rmse = np.sqrt(mse)\n",
    "  r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "  print(f'Mean Squared Error: {mse}')\n",
    "  print(f'Root Mean Squared Error: {rmse}')\n",
    "  print(f'R-squared: {r2}')\n",
    "\n",
    "  return rmse,r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_lin,r2_lin = linearReg(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Stochastic Gradient Descent Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgdRegressor(X_train, X_test, y_train, y_test, iterations):\n",
    "  model = SGDRegressor(max_iter=iterations)\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_test)\n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  rmse = np.sqrt(mse)\n",
    "  r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "  print(f'Mean Squared Error: {mse}')\n",
    "  print(f'Root Mean Squared Error: {rmse}')\n",
    "  print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdRegressor(X_train, X_test, y_train, y_test, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_X = X\n",
    "poly_features = PolynomialFeatures(degree=3, include_bias=False)\\\n",
    "                                            .fit_transform(small_X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression with polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearReg(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Regression with polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdRegressor(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Regression Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first reinitialize the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a regressor object \n",
    "regressor = DecisionTreeRegressor(random_state = 0)  \n",
    "  \n",
    "# fit the regressor with X and Y data \n",
    "regressor.fit(X_train, y_train) \n",
    "\n",
    "#test the regressor\n",
    "y_reg = regressor.predict(X_test)\n",
    "\n",
    "# evaluate model\n",
    "rmse_DT = mean_squared_error(y_test, y_reg, squared=False)\n",
    "r2_DT = r2_score(y_test, y_reg)\n",
    "print(f'RMSE= {rmse_DT}, R2= {r2_DT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "x_categorical = dataset_df.select_dtypes(include=['object']).apply(label_encoder.fit_transform)\n",
    "x_numerical = dataset_df.select_dtypes(exclude=['object']).values\n",
    "x = pd.concat([pd.DataFrame(x_numerical), x_categorical], axis=1).values\n",
    "\n",
    "# Fitting Random Forest Regression to the dataset\n",
    "regressor_rf = RandomForestRegressor(n_estimators=10, random_state=0, oob_score=True)\n",
    "\n",
    "# Fit the regressor with x and y data\n",
    "regressor_rf.fit(X_train, y_train)\n",
    "\n",
    "#test the regressor\n",
    "y_reg_rf = regressor_rf.predict(X_test)\n",
    "\n",
    "# evaluate model\n",
    "rmse_RF = mean_squared_error(y_test, y_reg_rf, squared=False)\n",
    "r2_RF = r2_score(y_test, y_reg_rf)\n",
    "print(f'RMSE= {rmse_RF}, R2= {r2_RF}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create a XGBRegressor instance\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror',random_state=42)\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_best = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "rmse_xgb = mean_squared_error(y_test, y_pred_best, squared=False)\n",
    "r2_xgb = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Matplotlib and Importance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get Importances from features\n",
    "feature_importances = best_xgb_model.feature_importances_\n",
    "\n",
    "# X axis\n",
    "feature_names = X.columns \n",
    "\n",
    "# Create a DataFrame\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "# Sort by value (descending)\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Plot graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results and Metrics\n",
    "\n",
    "### Feature importance\n",
    "We consider the feature importances of the gradient boosting model. By far the most important feature is the overall value which is a FIFA game specific value. The most important features after this one are potential, the remaining time the contract at the club is valid, the weak foot and shooting ability.\n",
    "\n",
    "### Model errors\n",
    "In the following plots we can see the Random Mean Squared Error and the R^2 values of all models compared. The linear regression performed the worst with RMSE of almost 0.5 and R^2 of around 0.8. Gradient boosting had a RMSE of around 0.1 and R^2 of almost 1. The Regression tree and Random Forest performed the best with an extremely low RMSE and R^2 even higher than the one of the Gradient Boosting model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot performance measures\n",
    "# Params: values as list, title, categories as list\n",
    "def plotMeasures(values, title, categories):\n",
    "    values = np.array(values)\n",
    "    bar_width = 0.2\n",
    "\n",
    "    for i in range(len(categories)):\n",
    "        plt.bar(i * bar_width, values[i], width=bar_width, label=categories[i])\n",
    "\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.xlabel('Model')\n",
    "    plt.title(title)\n",
    "\n",
    "\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMeasures([rmse_lin, rmse_DT, rmse_RF,rmse_xgb], 'RMSE', ['Linear Regression', 'Regression Tree', 'Random Forest','XGBoost'])\n",
    "plotMeasures([r2_lin, r2_DT, r2_RF,r2_xgb], 'R^2', ['Linear Regression', 'Regression Tree', 'Random Forest','XGBoost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We obtained really good results for predicting the market value of the FIFA players. Especially the Random Forest performed extraordinarily well. However we could also observe that the most important feature for the Gradient Boosting was the value 'overall' with an importance of over 0.6. This is a very specific ingame value that was again derived by developers from the player's statistics (such as also market price) just for the game. Considering this fact the very good results might be attributed to the fact that it's data for a video game. Hypothetically using the same approach on real life FIFA data only would maybe not lead to such low error rates. However, the general approach that we developed within this project can be used for similar predictions of continuos values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
